# ================== IMPORTS ==================
import time
import numpy as np
import pandas as pd


from collections import Counter, defaultdict
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# ================== LOAD DATA ==================
def load_data(path):
    start = time.time()
    df = pd.read_csv(path)
    print("Dataset Loaded:", df.shape, "| Time:", round(time.time() - start, 2), "s")
    return df


# ================== PREPROCESSING ==================
def preprocess_data(df):
    df.dropna(inplace=True)

    X = df.drop('label', axis=1)
    y = df['label']

    # Encode categorical features
    for col in X.select_dtypes(include=['object']).columns:
        X[col] = LabelEncoder().fit_transform(X[col])

    # Encode target labels
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Feature scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X).astype('float32')

    print("Preprocessing completed")
    return X_scaled, y_encoded, label_encoder


# ================== BALANCE DATASET ==================
def balance_dataset(X, y, max_majority=10000):
    class_counts = Counter(y)

    # Step 1: Undersampling majority classes
    under_strategy = {
        cls: max_majority for cls, count in class_counts.items()
        if count > max_majority
    }

    if under_strategy:
        rus = RandomUnderSampler(sampling_strategy=under_strategy, random_state=42)
        X_res, y_res = rus.fit_resample(X, y)
    else:
        X_res, y_res = X, y

    # Step 2: Oversample minority classes
    ros = RandomOverSampler(sampling_strategy='not majority', random_state=42)
    X_bal, y_bal = ros.fit_resample(X_res, y_res)

    print(" Balanced Class Distribution:", np.bincount(y_bal))
    return X_bal, y_bal


# ================== MODEL DEFINITIONS ==================
def get_models():
    dt = DecisionTreeClassifier(
        criterion='entropy',
        max_depth=20,
        class_weight='balanced',
        random_state=42
    )

    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=18,
        max_features='sqrt',
        class_weight='balanced_subsample',
        random_state=42,
        n_jobs=-1
    )

    knn = KNeighborsClassifier(
        n_neighbors=7,
        weights='distance'
    )

    return dt, rf, knn


# ================== TRAIN MODELS ==================
def train_models(models, X_train, y_train):
    for model, name in models:
        start = time.time()
        model.fit(X_train, y_train)
        print(f" Trained {name} in {round(time.time() - start, 2)} s")


# ================== EVALUATION ==================
def evaluate_models(models, X_test, y_test):
    predictions = {}

    print("\n=== Individual Model Accuracies ===")
    for model, name in models:
        y_pred = model.predict(X_test)
        predictions[name] = y_pred
        acc = accuracy_score(y_test, y_pred) * 100
        print(f"{name}: {acc:.3f} %")

    return predictions


# ================== ENSEMBLE MODEL ==================
def train_ensemble(dt, rf, knn, X_train, y_train):
    ensemble = VotingClassifier(
        estimators=[('dt', dt), ('rf', rf), ('knn', knn)],
        voting='soft',
        weights=[1, 1, 1],
        n_jobs=-1
    )

    start = time.time()
    ensemble.fit(X_train, y_train)
    print("\n Ensemble trained in", round(time.time() - start, 2), "s")
    return ensemble


# ================== PER-CLASS METRICS ==================
def print_class_metrics(y_test, y_pred, label_encoder):
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred)

    print("\n=== Per-Class Metrics ===")
    for i, cls in enumerate(label_encoder.classes_):
        print(
            f"{cls}: "
            f"Precision={precision[i]*100:.2f}% | "
            f"Recall={recall[i]*100:.2f}% | "
            f"F1={f1[i]*100:.2f}%"
        )


# ================== GROUPED ATTACK ACCURACY ==================
def grouped_accuracy(y_test, predictions, label_encoder):
    attack_groups = {
        'DDoS-ACK_Fragmentation': 'DoS', 'DDoS-HTTP_Flood': 'DoS',
        'DDoS-ICMP_Flood': 'DoS', 'DDoS-SYN_Flood': 'DoS',
        'DoS-TCP_Flood': 'DoS', 'DoS-UDP_Flood': 'DoS',

        'Recon-HostDiscovery': 'Probe', 'Recon-PortScan': 'Probe',

        'SqlInjection': 'R2L', 'XSS': 'R2L',

        'Mirai-udpplain': 'U2R',

        'Backdoor_Malware': 'Malware',

        'VulnerabilityScan': 'VulnerabilityScan',
        'MITM-ArpSpoofing': 'MITM',
        'BenignTraffic': 'Benign'
    }

    class_to_group = {
        i: attack_groups.get(c, 'Other')
        for i, c in enumerate(label_encoder.classes_)
    }

    results = defaultdict(dict)

    for model_name, y_pred in predictions.items():
        for grp in ['DoS','Probe','R2L','U2R','Malware',
                    'VulnerabilityScan','MITM','Benign','Other']:

            idx = [i for i, lbl in enumerate(y_test)
                   if class_to_group[lbl] == grp]

            acc = np.mean(y_pred[idx] == y_test[idx]) * 100 if idx else np.nan
            results[model_name][grp] = round(acc, 3)

    df = pd.DataFrame(results).T
    print("\n=== Group-wise Accuracy (%) ===")
    print(df.to_string())


# ================== MAIN ==================
def main():
    df = load_data("Dataset.csv.csv")

    X, y, le = preprocess_data(df)
    X_bal, y_bal = balance_dataset(X, y)

    X_train, X_test, y_train, y_test = train_test_split(
        X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42
    )

    dt, rf, knn = get_models()
    models = [(dt, "Decision Tree"), (rf, "Random Forest"), (knn, "KNN")]

    train_models(models, X_train, y_train)
    predictions = evaluate_models(models, X_test, y_test)

    ensemble = train_ensemble(dt, rf, knn, X_train, y_train)
    y_ensemble = ensemble.predict(X_test)

    print("\n=== Ensemble Accuracy ===")
    print("Accuracy:", accuracy_score(y_test, y_ensemble) * 100)

    print_class_metrics(y_test, y_ensemble, le)

    predictions["Voting Classifier"] = y_ensemble
    grouped_accuracy(y_test, predictions, le)


if __name__ == "__main__":
    main()
